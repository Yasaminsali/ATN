{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from combat.pycombat import pycombat\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read every cohort study file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../cohort_studies_full_data/' + \"/*.\"+'csv'))]\n",
    "cohorts = [file.split(\".\")[0] for file in sorted(os.listdir('../cohort_studies_full_data/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies = dict()\n",
    "# dfsss = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts, datasets):\n",
    "    cohort_n = cohort.split(\"_MERGE\")[0]\n",
    "    cohort_studies[cohort_n] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit\n",
    "#     dfsss[cohort_n] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_sub = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../preprocessed_datasets/' + \"/*.\"+'csv'))]\n",
    "cohorts_sub = [file.split(\".\")[0] for file in sorted(os.listdir('../preprocessed_datasets/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies_sub = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts_sub, datasets_sub):\n",
    "    cohort_studies_sub[cohort] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the preprocessed columns from sub table of each dataset to the main table of dataset\n",
    "for i in cohort_studies:\n",
    "#     cols = cohort_studies_sub[i].columns.difference(cohort_studies[i].columns)\n",
    "    cols = ['Age', 'Sex', 'Education', 'APOE4', 'CDR', 'Race']\n",
    "    \n",
    "    for col in cols:\n",
    "        \n",
    "        if col in cohort_studies_sub[i].columns:\n",
    "            cohort_studies[i][col] = cohort_studies_sub[i][col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read harmonized mapping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = [pd.read_csv(file, sep=',') for file in sorted(glob.glob('../feature_tables' + \"/*.\"+'csv'))]\n",
    "name = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_tables'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "mappings = dict()\n",
    "\n",
    "for moda, na in zip(modality, name):\n",
    "    mappings[na.split(' - ')[1]] = moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_features = pd.concat(mappings, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude categorical and taboo features\n",
    "harmonized_features = harmonized_features.loc[(harmonized_features['Rank']!=1) & (harmonized_features['Rank']!=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the feature availability files for all cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_mapp = [pd.read_csv(file, sep='\\t') for file in sorted(glob.glob('../feature_availability_in_cohorts' + \"/*.\"+'tsv'))]\n",
    "tablesss = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_availability_in_cohorts'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "available_features = dict()\n",
    "\n",
    "for modal, df in zip(tablesss, ava_mapp):\n",
    "    available_features[modal] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features = pd.concat(available_features, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features.replace({0: np.nan}, inplace=True) # 0 indicates that the feature was not measured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read cutoffs obtained using all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_method = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../results/cutoffs/' + \"/*.\"+'csv'))]\n",
    "method_name = [file.split(\".\")[0] for file in sorted(os.listdir('../results/cutoffs/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains each cutoff table as a dataframe\n",
    "cutoffs_ = dict()\n",
    "\n",
    "for tm, mn in zip(table_method, method_name):\n",
    "    cutoffs_[mn] = tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cutoffs_['km_cutoffs'].rename(columns={col: col.split('_')[0]}, inplace=True) for col in cutoffs_['km_cutoffs'].columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecetion of cohort studies for A/T/N assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the patient that have CSF biomarker, disregard the diagnostic status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "atn = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=mappings['csf'].Feature.loc[0:2].to_list()+([\"Total\"]))\n",
    "# atn = pd.DataFrame(index=cohort_studies, columns=['A', 'T', 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    for feat in mappings['csf'][cohort].loc[0:2].dropna().to_list():\n",
    "        if feat in cohort_studies[cohort].columns:\n",
    "            atn.loc[cohort, mappings['csf'].loc[mappings['csf'][cohort]==feat, 'Feature']] = len(cohort_studies[cohort][feat].dropna())\n",
    "            atn.loc[cohort, 'Total'] = len(cohort_studies[cohort][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=cohort_studies['ADNI']['Diagnosis'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in diag.index:\n",
    "    for dia in diag.columns:\n",
    "        diag.loc[cohort, dia] = len(cohort_studies[cohort].loc[cohort_studies[cohort]['Diagnosis']==dia][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the empty columns from all cohorts that we are intrested in\n",
    "### Remove the participant without all 3 CSF biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cohorts = dict()\n",
    "\n",
    "for coh in diag.index:\n",
    "    selected_cohorts[coh] = cohort_studies[coh].dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feats = dict()\n",
    "\n",
    "# existing_features.set_index('Feature', inplace=True)\n",
    "\n",
    "for feat in existing_features.Feature:\n",
    "    total_feats[feat] = existing_features.loc[existing_features.Feature==feat][selected_cohorts].dropna(axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    feat = mappings['csf'][cohort].loc[0:2].dropna().to_list()\n",
    "    cohort_studies[cohort] = cohort_studies[cohort].dropna(subset=feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Some features have suffix due to merging tables for certain cohorts, first investigate if all the harmonized features are in cohorts. Rename the ones that have suffix so it can be compatible to work with our harmonized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_studies['ADNI'] = cohort_studies['ADNI'].rename(columns={'PTEDUCAT_x': 'PTEDUCAT', 'TRABSCOR_bl': 'TRABSCOR', 'LDELTOTAL_BL': 'LDELTOTAL'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSF biomarkers, two classes, normal vs abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modality_ = ['clinical_i', 'clinical_ii','hippocampus', 'csf']\n",
    "# modality_ = ['basal_ganglia', 'brain_poles_volume', 'cerebellum', 'clinical_i', \n",
    "#             'clinical_ii', 'csf', 'diencephalus', 'general_brain', \n",
    "#             'mri_others', 'pet', 'plasma', 'ventricles', 'hippocampus']\n",
    "modality_ = ['basal_ganglia', 'brain_poles_volume', 'cerebellum', 'csf', 'diencephalus', 'general_brain', \n",
    "            'mri_others', 'pet', 'plasma', 'ventricles', 'hippocampus']\n",
    "\n",
    "selected_feat = dict()\n",
    "\n",
    "for i in modality_:\n",
    "    selected_feat[i] = mappings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe containing all the mapped features\n",
    "features_all = pd.concat(selected_feat, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = features_all[atn.index.union(['Feature', 'Rank'])] # subset the cohorts of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the features that are not available in all studies\n",
    "features_all = features_all.loc[features_all['Feature'].isin(existing_features[atn.index.union(['Feature'])].dropna(how='all')['Feature'].to_list())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert MRI measurements to mm3, same as ADNI and other cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_nacc = ['basal_ganglia', 'brain_poles_volume', 'cerebellum', 'diencephalus', 'general_brain', \n",
    "            'mri_others', 'pet', 'plasma', 'ventricles', 'hippocampus']\n",
    "\n",
    "for i in mri_nacc:\n",
    "    \n",
    "    for variable in mappings[i][['Feature', 'NACC']].dropna()['Feature'].to_list():\n",
    "        \n",
    "        if \"Volume\" in variable:\n",
    "            nacc_var = mappings[i].loc[mappings[i]['Feature']==variable, 'NACC']\n",
    "            cohort_studies['NACC'][nacc_var] = cohort_studies['NACC'][nacc_var] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nacc_mri_to_convert = list()\n",
    "\n",
    "for i in mri_nacc:\n",
    "    \n",
    "    for feat_ in mappings[i]['NACC'].dropna().to_list():\n",
    "        nacc_mri_to_convert.append(feat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort_studies['NACC'][nacc_mri_to_convert].dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rank 1 --> Categorical features\n",
    "* Rank 2 --> Taboo features: some categorical and some numerical\n",
    "* Rank nan --> Numerical features \n",
    "\n",
    "replace nan with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all['Rank'].replace({np.nan: 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all.replace({\"No total score.\": np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix PharmaCog column names\n",
    "for i in cohort_studies['PharmaCog'].columns:\n",
    "    if \"\\xa0\" in i:\n",
    "        new = str(i).replace(u'\\xa0', u'')\n",
    "        cohort_studies['PharmaCog'].rename(columns={i: new}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'MCI': 145})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(cohort_studies['PharmaCog']['Diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_atn_participants(dfss, cohorts, thresholds, features):\n",
    "    \"\"\"cohorts: list of cohort names \n",
    "       dfss: dictionary of cohorts where each key is the name of a cohort\n",
    "       thresholds: cutoff values obtained using a methodology\n",
    "       \n",
    "       Select the features and participants and categorize the participant into ATN profiles using thresholds\n",
    "       obtained from each methodology.\n",
    "       \n",
    "       return a df which contain the combination of paticipant from the selected cohorts while harmonizing\n",
    "       the features names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # make a list of additional features to be investigated \n",
    "#     additional_feat = ['Age', 'Sex', 'Education', 'APOE4', 'CDR', 'Race']\n",
    "    additional_feat = ['APOE4']\n",
    "\n",
    "    # make an empty dictionary to add the datasets to\n",
    "    df_= pd.DataFrame(columns=set(features.loc[features['Rank']==0]\n",
    "                                  [cohorts + ['Feature']].dropna()['Feature']).difference(['Feature'] + list(thresholds.columns)).union(additional_feat))\n",
    "   \n",
    "    for i in cohorts:\n",
    "#         print(i)\n",
    "\n",
    "        dfs = dict()\n",
    "        dfs[i] = dfss[i].copy() # make a copy of the dataset of interest\n",
    "        # select the subset of datasets with features to be investigated\n",
    "        dfs[i] = dfs[i][features.loc[features['Rank']==0][cohorts + ['Feature']].dropna()[i].to_list() + additional_feat].dropna(axis=1, how='all')\n",
    "        # rename all the columns so we can concat the datasets later\n",
    "        [dfs[i].rename(columns={col: coln}, inplace=True) for col, coln in \n",
    "         zip(features.loc[features['Rank']==0][cohorts + ['Feature']].dropna()[i].to_list(), \n",
    "             features.loc[features['Rank']==0][cohorts + ['Feature']].dropna()['Feature'].to_list())]\n",
    "        dfs[i]['Cohort'] = i # add a cohort name column\n",
    "        # change the datatype to str as these are categorical features. astype doesn't work as it will include nan values\n",
    "        dfs[i] = dfs[i].replace({'APOE4': {0.0: '0', 2.0: '2', 1.0: '1'}}) \n",
    "\n",
    "        if i!='NACC':\n",
    "            \n",
    "            for biomarker in thresholds.columns:\n",
    "                # select the cutoff value for each biomarker for each cohort\n",
    "                threshold = thresholds.loc[i][biomarker]\n",
    "\n",
    "                # dichotomize the participants\n",
    "                if biomarker == 'pTau in CSF': \n",
    "                    dfs[i].loc[dfs[i][biomarker]>threshold, \"T\"] = 'T+'\n",
    "                    dfs[i].loc[dfs[i][biomarker]<threshold, \"T\"] = 'T-'\n",
    "\n",
    "                elif biomarker == 'tTau in CSF': \n",
    "                    dfs[i].loc[dfs[i][biomarker]>threshold, \"N\"] = 'N+'\n",
    "                    dfs[i].loc[dfs[i][biomarker]<threshold, \"N\"] = 'N-'\n",
    "\n",
    "                else: \n",
    "                    dfs[i].loc[dfs[i][biomarker]<threshold, \"A\"] = 'A+'\n",
    "                    dfs[i].loc[dfs[i][biomarker]>threshold, \"A\"] = 'A-'\n",
    "\n",
    "            # join the 3 columns to make the final ATN categorie                                 \n",
    "            dfs[i]['ATN'] = dfs[i]['A'] + dfs[i]['T'] + dfs[i]['N']\n",
    "            # remove the columns that we are not interested in\n",
    "            dfs[i] = dfs[i][dfs[i].columns.difference(['A', 'T', 'N', 'Mini-Mental State Examination (MMSE)'] + list(thresholds.columns))]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            elisa_index = cohort_studies[i].loc[(cohort_studies[i]['CSFTTMD']==1)].index\n",
    "            xmap_index = cohort_studies[i].loc[(cohort_studies[i]['CSFTTMD']==2)].index\n",
    "            \n",
    "            for biomarker in thresholds.columns:\n",
    "                # select the cutoff value for each biomarker for each cohort\n",
    "                elisa = thresholds.loc[i + \"_ELISA\"][biomarker] # ELISA\n",
    "                xmap = thresholds.loc[i + \"_XMAP\"][biomarker] #XMAP\n",
    "\n",
    "                # dichotomize the participants\n",
    "                if biomarker == 'pTau in CSF': \n",
    "                    \n",
    "                    dfs[i].loc[(dfs[i].index.isin(elisa_index)) & (dfs[i][biomarker]>elisa), \"T\"] = 'T+'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(elisa_index)) & (dfs[i][biomarker]<elisa), \"T\"] = 'T-'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(xmap_index)) & (dfs[i][biomarker]>xmap), \"T\"] = 'T+'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(xmap_index)) & (dfs[i][biomarker]<xmap), \"T\"] = 'T-'\n",
    "\n",
    "                elif biomarker == 'tTau in CSF': \n",
    "                    dfs[i].loc[(dfs[i].index.isin(elisa_index)) & (dfs[i][biomarker]>elisa), \"N\"] = 'N+'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(elisa_index)) & (dfs[i][biomarker]<elisa), \"N\"] = 'N-'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(xmap_index)) & (dfs[i][biomarker]>xmap), \"N\"] = 'N+'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(xmap_index)) & (dfs[i][biomarker]<xmap), \"N\"] = 'N-'\n",
    "\n",
    "                else: \n",
    "                    dfs[i].loc[(dfs[i].index.isin(elisa_index)) & (dfs[i][biomarker]<elisa), \"A\"] = 'A+'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(elisa_index)) & (dfs[i][biomarker]>elisa), \"A\"] = 'A-'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(xmap_index)) & (dfs[i][biomarker]<xmap), \"A\"] = 'A+'\n",
    "                    dfs[i].loc[(dfs[i].index.isin(xmap_index)) & (dfs[i][biomarker]>xmap), \"A\"] = 'A-'\n",
    "\n",
    "            # join the 3 columns to make the final ATN categorie                                 \n",
    "            dfs[i]['ATN'] = dfs[i]['A'] + dfs[i]['T'] + dfs[i]['N']\n",
    "            # remove the columns that we are not interested in\n",
    "            dfs[i] = dfs[i][dfs[i].columns.difference(['A', 'T', 'N', 'Mini-Mental State Examination (MMSE)', 'Trail Making Test (TMT) A', 'Verbal fluency tests (Semantic) Animal'] + list(thresholds.columns))]\n",
    "            \n",
    "#         print(dfs[i])\n",
    "\n",
    "        \n",
    "        df_ = pd.concat([df_, dfs[i]])\n",
    "#         print(df_)\n",
    "#         df_ = df_.dropna(axis=1, thresh=len(df_.index)/2)\n",
    "#         print(df_)\n",
    "    \n",
    "    df_ = df_.dropna(axis=1, how='all')\n",
    "    df_ = df_.dropna(axis=1, thresh=1200)\n",
    "\n",
    "    df_ = df_.dropna()\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_df_km = select_atn_participants(cohort_studies, ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog'], cutoffs_['km_cutoffs'], features_all)\n",
    "clustering_df_gmm = select_atn_participants(cohort_studies, ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog'], cutoffs_['gmm_cutoffs'], features_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exclude CSF Volume as it seems wrong in the NACC dataset and that could potentially add bias to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_df_km = clustering_df_km[clustering_df_km.columns.difference(['Cerebrospinal Fluid Volume'])]\n",
    "clustering_df_gmm = clustering_df_gmm[clustering_df_gmm.columns.difference(['Cerebrospinal Fluid Volume'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch effect correstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n",
      "Adjusting the Data\n",
      "Found 7 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n",
      "Adjusting the Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "km_corrected = pycombat(clustering_df_km.drop(columns=['ATN', 'Cohort', 'APOE4']).transpose(), \n",
    "         batch=clustering_df_km['Cohort']).transpose()\n",
    "\n",
    "gmm_corrected = pycombat(clustering_df_gmm.drop(columns=['ATN', 'Cohort', 'APOE4']).transpose(), \n",
    "         batch=clustering_df_gmm['Cohort']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_corrected[['ATN', 'Cohort', 'APOE4']] = clustering_df_km[['ATN', 'Cohort', 'APOE4']]\n",
    "gmm_corrected[['ATN', 'Cohort', 'APOE4']] = clustering_df_gmm[['ATN', 'Cohort', 'APOE4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_corrected.to_csv(\"../results/batch_effect_corrected_dfs/gmm.csv\")\n",
    "km_corrected.to_csv(\"../results/batch_effect_corrected_dfs/km.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with PdfPages('../results/boxplot_batch_correction/batch_effect.pdf') as pdf:\n",
    "\n",
    "#     for i in clustering_df_km.columns.difference(['ATN', 'APOE4', 'Cohort']):\n",
    "#         fig, axes = plt.subplots(1,2, figsize=(20, 8))\n",
    "\n",
    "#         sns.boxplot(data=clustering_df_km, x='Cohort', y=i, ax=axes[0])\n",
    "#         sns.boxplot(data=km_corrected, x='Cohort', y=i, ax=axes[1])\n",
    "#         axes[0].set_title(\"Raw measurements\")\n",
    "#         axes[1].set_title(\"Corrected measurements\")\n",
    "\n",
    "#         fig.get_figure()\n",
    "#         pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with PdfPages('../results/boxplot_batch_correction/batch_effect_gmm.pdf') as pdf:\n",
    "\n",
    "#     for i in clustering_df_km.columns.difference(['ATN', 'APOE4', 'Cohort']):\n",
    "#         fig, axes = plt.subplots(1,2, figsize=(20, 8))\n",
    "\n",
    "#         sns.boxplot(data=clustering_df_gmm, x='Cohort', y=i, ax=axes[0])\n",
    "#         sns.boxplot(data=gmm_corrected, x='Cohort', y=i, ax=axes[1])\n",
    "#         axes[0].set_title(\"Raw measurements\")\n",
    "#         axes[1].set_title(\"Corrected measurements\")\n",
    "\n",
    "#         fig.get_figure()\n",
    "#         pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atn_based_df(df):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    labels_atn = dict()\n",
    "    score_atn = dict()\n",
    "    \n",
    "    # change the cohort names to int and write in a column\n",
    "    for i, j in zip(df.Cohort.unique(), range(len(df.Cohort.unique()))):\n",
    "        df.loc[df['Cohort']==i, 'Cohort_number'] = j\n",
    "    \n",
    "    # select the profiles that have over 20 participants\n",
    "    # make a dictionary where the selcted ATN profiles are key and \n",
    "    # the subset of dataframe categorized in that ATN profile is value\n",
    "    dfs_sub = {atn: df.loc[df.ATN==atn].copy() for atn in list((a) for a,b in dict(Counter(df.ATN)).items() if b >20)}\n",
    "    \n",
    "    for i in dfs_sub:\n",
    "#         print(i, len(dfs_sub[i].Cohort.unique()))\n",
    "        hierarchical_cluster = AgglomerativeClustering(n_clusters=len(dfs_sub[i].Cohort.unique()), affinity='euclidean', linkage='ward')\n",
    "        labels = hierarchical_cluster.fit_predict(dfs_sub[i][dfs_sub[i].columns.difference(['ATN', 'Cohort', 'Cohort_number'])])\n",
    "        labels_atn[i] = labels\n",
    "        score_atn[i] = purity_score(dfs_sub[i]['Cohort_number'], labels)\n",
    "     \n",
    "    return labels_atn, score_atn, dfs_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_km, score_km, atn_km_df = atn_based_df(clustering_df_km)\n",
    "labels_gmm, score_gmm, atn_gmm_df = atn_based_df(clustering_df_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "labels_km_t, score_km_t, atn_km_df_t = atn_based_df(km_corrected)\n",
    "labels_gmm_t, score_gmm_t, atn_gmm_df_t = atn_based_df(gmm_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Cluster purity (K-means):\")\n",
    "# score_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Cluster purity (GMM):\")\n",
    "# score_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in labels_km:\n",
    "    atn_km_df[i].loc[:, 'predicted_cohort'] = list(labels_km[i])\n",
    "\n",
    "for i_ in labels_gmm:\n",
    "    atn_gmm_df[i_].loc[:, 'predicted_cohort'] = list(labels_gmm[i_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "for i in labels_km_t:\n",
    "    atn_km_df_t[i].loc[:, 'predicted_cohort'] = list(labels_km_t[i])\n",
    "\n",
    "for i_ in labels_gmm_t:\n",
    "    atn_gmm_df_t[i_].loc[:, 'predicted_cohort'] = list(labels_gmm_t[i_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cramer(dfs):\n",
    "    \"\"\" \"\"\"\n",
    "    results = dict()\n",
    "    \n",
    "    for i in dfs:\n",
    "        df_ = dfs[i].set_index('Cohort')\n",
    "        mat = pd.crosstab(df_.index, df_['predicted_cohort'])\n",
    "#         print(i, mat)\n",
    "#         print(i, round(stats.contingency.association(mat, method='cramer'), 2))\n",
    "        results[i] = round(stats.contingency.association(mat, method='cramer'), 2)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(calculate_cramer(atn_km_df), orient='index').transpose()[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(calculate_cramer(atn_gmm_df), orient='index').transpose()[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the batch-corrected ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramer_km = pd.DataFrame.from_dict(calculate_cramer(atn_km_df_t), orient='index').transpose()[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramer_gmm = pd.DataFrame.from_dict(calculate_cramer(atn_gmm_df_t), orient='index').transpose()[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip([cramer_km, cramer_gmm], ['K-Means', 'GMM']):\n",
    "    i.rename(index={0: j}, inplace=True)\n",
    "    i.to_csv(f\"../results/clustering/cramers_v/cramers_v_{j}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chi2(dfs):\n",
    "    \"\"\" \"\"\"\n",
    "    results_chi2 = dict()\n",
    "    results_p = dict()\n",
    "    results_df = dict()\n",
    "    results_ex = dict()\n",
    "    \n",
    "    for i in dfs:\n",
    "        df_ = dfs[i].set_index('Cohort')\n",
    "        mat = pd.crosstab(df_.index, df_['predicted_cohort'])\n",
    "        results_chi2[i], results_p[i], results_df[i], results_ex[i] = stats.chi2_contingency(mat, correction=True)\n",
    "        \n",
    "    return results_chi2, results_p, results_df, results_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_chi2_km, results_p_km, results_df_km, results_ex_km = calculate_chi2(atn_km_df)\n",
    "# results_chi2_gmm, results_p_gmm, results_df_gmm, results_ex_gmm = calculate_chi2(atn_gmm_df)\n",
    "\n",
    "# corrected dataframes\n",
    "results_chi2_km_t, results_p_km_t, results_df_km_t, results_ex_km_t = calculate_chi2(atn_km_df_t)\n",
    "results_chi2_gmm_t, results_p_gmm_t, results_df_gmm_t, results_ex_gmm_t = calculate_chi2(atn_gmm_df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A-T-N-': 47.2738997828431,\n",
       " 'A+T-N-': 39.86569318064662,\n",
       " 'A+T+N+': 50.74756679162755,\n",
       " 'A-T+N+': 12.767045454545453,\n",
       " 'A+T+N-': 24.40691661279897,\n",
       " 'A+T-N+': 33.557491522843705,\n",
       " 'A-T-N+': 25.86746031746032}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_chi2_gmm_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_km = pd.DataFrame.from_dict(results_p_km_t, orient='index').transpose()[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_gmm = pd.DataFrame.from_dict(results_p_gmm_t, orient='index').transpose()[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Cramer's V and P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A-T-N-</th>\n",
       "      <th>A-T+N+</th>\n",
       "      <th>A-T-N+</th>\n",
       "      <th>A+T+N-</th>\n",
       "      <th>A+T-N-</th>\n",
       "      <th>A+T-N+</th>\n",
       "      <th>A+T+N+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A-T-N-  A-T+N+  A-T-N+  A+T+N-  A+T-N-  A+T-N+  A+T+N+\n",
       "0     0.1    0.69    0.41    0.08     0.3    0.12    0.05"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip([pvalue_km, pvalue_gmm], ['K-Means', 'GMM']):\n",
    "    i.rename(index={0: j}, inplace=True)\n",
    "    i.to_csv(f\"../results/clustering/p_value/pvalue_{j}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: One could use the crosstab instead of the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clusters(df):\n",
    "    \n",
    "    \"\"\"df: dataframe with ATN categorization using certain method and containing cluster labels\n",
    "       return: the number of participant within each cohort and within each ATN profile assigned to each labels\"\"\"\n",
    "    \n",
    "    # make an empty dictionary of dataframes to store the results\n",
    "    clustering_result_ = {i: pd.DataFrame(index=df[i].Cohort.unique(), \n",
    "                                            columns=sorted(df[i].predicted_cohort.unique())) for i in df}\n",
    "    \n",
    "    # check the number of participants clustered to each coohort within each biomarker profile\n",
    "    for profi in df:\n",
    "    \n",
    "        for name in df[profi]['Cohort'].unique():\n",
    "            clustering_result_[profi].loc[name] = Counter(df[profi].loc[df[profi]['Cohort']==name, 'predicted_cohort'])\n",
    "            \n",
    "            \n",
    "    # replace all nan enteries with 0\n",
    "    [clustering_result_[i].replace({np.nan: 0}, inplace=True) for i in clustering_result_]\n",
    "    \n",
    "    # change enteries to integer\n",
    "    for i in clustering_result_:\n",
    "        clustering_result_[i] = clustering_result_[i].astype(int)\n",
    "\n",
    "    return clustering_result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_result_km = count_clusters(atn_km_df_t)\n",
    "clustering_result_gmm = count_clusters(atn_gmm_df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means\n",
      "A-T-N- 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "A-T+N+ 6 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'DOD-ADNI', 'PharmaCog']\n",
      "A-T-N+ 6 ['ADNI', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "A+T+N- 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "A+T-N- 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "A+T-N+ 6 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'PharmaCog']\n",
      "A+T+N+ 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "GMM\n",
      "A-T-N- 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "A-T+N+ 5 ['ADNI', 'ARWIBO', 'NACC', 'DOD-ADNI', 'PharmaCog']\n",
      "A-T-N+ 6 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'PharmaCog']\n",
      "A+T+N- 5 ['ADNI', 'EDSD', 'NACC', 'JADNI', 'PharmaCog']\n",
      "A+T-N- 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n",
      "A+T-N+ 6 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'PharmaCog']\n",
      "A+T+N+ 7 ['ADNI', 'EDSD', 'ARWIBO', 'NACC', 'JADNI', 'DOD-ADNI', 'PharmaCog']\n"
     ]
    }
   ],
   "source": [
    "print(\"K-Means\")\n",
    "for i in ['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']:\n",
    "    print(i, len(clustering_result_km[i].index), list(clustering_result_km[i].index))\n",
    "    \n",
    "print(\"GMM\")\n",
    "for i in ['A-T-N-', 'A-T+N+', 'A-T-N+', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']:\n",
    "    print(i, len(clustering_result_gmm[i].index), list(clustering_result_gmm[i].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['A-T-N-', 'A+T-N-', 'A+T+N+', 'A-T+N+', 'A+T+N-', 'A-T-N+', 'A+T-N+'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_result_km.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['A-T-N-', 'A+T-N-', 'A+T+N+', 'A-T+N+', 'A+T+N-', 'A+T-N+', 'A-T-N+'])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_result_gmm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADNI</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDSD</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARWIBO</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NACC</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JADNI</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PharmaCog</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1  2  3  4  5\n",
       "ADNI       3  2  0  1  0  2\n",
       "EDSD       2  4  0  1  1  0\n",
       "ARWIBO     2  4  2  3  0  3\n",
       "NACC       9  4  6  2  5  1\n",
       "JADNI      0  3  1  0  0  1\n",
       "PharmaCog  0  1  0  0  1  3"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_result_gmm['A+T-N+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clustering_result_gmm:\n",
    "    clustering_result_gmm[i].to_csv(f\"../results/clustering/gmm/{i}_gmm.csv\")\n",
    "    clustering_result_km[i].to_csv(f\"../results/clustering/k_means/{i}_km.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramers' V interpretation based on degrees of freedom\n",
    "#### Degrees of freedom is calculated --> min(row-1, column-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers_df = pd.DataFrame(index=list(range(1,8)), columns=['s', 'm', 'l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers_df.iloc[0] = [0.1, 0.3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cramers_df.iloc[0]['m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cramers_df.iloc[1:].index:\n",
    "    \n",
    "    for col in cramers_df.columns:\n",
    "        \n",
    "        cramers_df.loc[i, col] = round(cramers_df.iloc[0][col] / math.sqrt(i), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>m</th>\n",
       "      <th>l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.041</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       s      m      l\n",
       "1    0.1    0.3    0.5\n",
       "2  0.071  0.212  0.354\n",
       "3  0.058  0.173  0.289\n",
       "4   0.05   0.15   0.25\n",
       "5  0.045  0.134  0.224\n",
       "6  0.041  0.122  0.204\n",
       "7  0.038  0.113  0.189"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cramers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
