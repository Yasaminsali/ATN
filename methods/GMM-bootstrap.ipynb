{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from scipy.stats import norm\n",
    "# from scipy.stats import bootstrap\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read every cohort study file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../cohort_studies_full_data/' + \"/*.\"+'csv'))]\n",
    "cohorts = [file.split(\".\")[0] for file in sorted(os.listdir('../cohort_studies_full_data/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies = dict()\n",
    "# dfsss = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts, datasets):\n",
    "    cohort_n = cohort.split(\"_MERGE\")[0]\n",
    "    cohort_studies[cohort_n] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit\n",
    "#     dfsss[cohort_n] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_sub = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../preprocessed_datasets/' + \"/*.\"+'csv'))]\n",
    "cohorts_sub = [file.split(\".\")[0] for file in sorted(os.listdir('../preprocessed_datasets/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies_sub = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts_sub, datasets_sub):\n",
    "    cohort_studies_sub[cohort] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cohort_studies:\n",
    "    cohort_studies[i]['Age']=cohort_studies_sub[i]['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read harmonized mapping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = [pd.read_csv(file, sep=',') for file in sorted(glob.glob('../feature_tables' + \"/*.\"+'csv'))]\n",
    "name = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_tables'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "mappings = dict()\n",
    "\n",
    "for moda, na in zip(modality, name):\n",
    "    mappings[na.split(' - ')[1]] = moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_features = pd.concat(mappings, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude categorical and taboo features\n",
    "harmonized_features = harmonized_features.loc[(harmonized_features['Rank']!=1) & (harmonized_features['Rank']!=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the feature availability files for all cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_mapp = [pd.read_csv(file, sep='\\t') for file in sorted(glob.glob('../feature_availability_in_cohorts' + \"/*.\"+'tsv'))]\n",
    "tablesss = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_availability_in_cohorts'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "available_features = dict()\n",
    "\n",
    "for modal, df in zip(tablesss, ava_mapp):\n",
    "    available_features[modal] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features = pd.concat(available_features, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features.replace({0: np.nan}, inplace=True) # 0 indicates that the feature was not measured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecetion of cohort studies for A/T/N assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the patient that have CSF biomarker, disregard the diagnostic status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "atn = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=mappings['csf'].Feature.loc[0:2].to_list()+([\"Total\"]))\n",
    "# atn = pd.DataFrame(index=cohort_studies, columns=['A', 'T', 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    for feat in mappings['csf'][cohort].loc[0:2].dropna().to_list():\n",
    "        if feat in cohort_studies[cohort].columns:\n",
    "            atn.loc[cohort, mappings['csf'].loc[mappings['csf'][cohort]==feat, 'Feature']] = len(cohort_studies[cohort][feat].dropna())\n",
    "            atn.loc[cohort, 'Total'] = len(cohort_studies[cohort][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=cohort_studies['ADNI']['Diagnosis'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in diag.index:\n",
    "    for dia in diag.columns:\n",
    "        diag.loc[cohort, dia] = len(cohort_studies[cohort].loc[cohort_studies[cohort]['Diagnosis']==dia][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the empty columns from all cohorts that we are intrested in\n",
    "### Remove the participant without all 3 CSF biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cohorts = dict()\n",
    "\n",
    "for coh in diag.index:\n",
    "    selected_cohorts[coh] = cohort_studies[coh].dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feats = dict()\n",
    "\n",
    "# existing_features.set_index('Feature', inplace=True)\n",
    "\n",
    "for feat in existing_features.Feature:\n",
    "    total_feats[feat] = existing_features.loc[existing_features.Feature==feat][selected_cohorts].dropna(axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    feat = mappings['csf'][cohort].loc[0:2].dropna().to_list()\n",
    "    cohort_studies[cohort] = cohort_studies[cohort].dropna(subset=feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Some features have suffix due to merging tables for certain cohorts, first investigate if all the harmonized features are in cohorts. Rename the ones that have suffix so it can be compatible to work with our harmonized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_studies['ADNI'].rename(columns={'PTEDUCAT_x': 'PTEDUCAT', 'TRABSCOR_bl': 'TRABSCOR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering CSF biomarkers, two classes, normal vs abnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset each cohort dataset based on the columns of interest for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts_csf = dict()\n",
    "\n",
    "for i in atn.index:\n",
    "    csf = mappings['csf'].iloc[:3][i].to_list()\n",
    "    \n",
    "    if i == 'NACC':\n",
    "        cohorts_csf['NACC_ELISA'] = cohort_studies[i].loc[cohort_studies[i]['CSFTTMD']==1][csf + [\"Diagnosis\", \"Age\"]] # ELISA\n",
    "        cohorts_csf['NACC_XMAP'] = cohort_studies[i].loc[cohort_studies[i]['CSFTTMD']==2][csf + [\"Diagnosis\", \"Age\"]] # xmap\n",
    "        cohorts_csf['NACC_ELISA'] = cohorts_csf['NACC_ELISA'].dropna(subset=cohorts_csf['NACC_ELISA'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "        cohorts_csf['NACC_XMAP'] = cohorts_csf['NACC_XMAP'].dropna(subset=cohorts_csf['NACC_XMAP'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "\n",
    "    \n",
    "    elif i == 'EMIF':\n",
    "        cohorts_csf['EMIF_ELISA'] = cohort_studies[i].loc[~(cohort_studies[i]['Studyname'].isin(['EDAR', 'Leuven', ]))][csf + [\"Diagnosis\", \"Age\"]] # INNOTEST ELISA\n",
    "        cohorts_csf['EMIF_XMAP'] = cohort_studies[i].loc[(cohort_studies[i]['Studyname'].isin(['EDAR', 'Leuven', ]))][csf + [\"Diagnosis\", \"Age\"]] # xmap and not collected\n",
    "        cohorts_csf['EMIF_ELISA'] = cohorts_csf['EMIF_ELISA'].dropna(subset=cohorts_csf['EMIF_ELISA'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "        cohorts_csf['EMIF_XMAP'] = cohorts_csf['EMIF_XMAP'].dropna(subset=cohorts_csf['EMIF_XMAP'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "\n",
    "    else: \n",
    "        cohorts_csf[i] = cohort_studies[i][csf + [\"Diagnosis\", \"Age\"]]\n",
    "        cohorts_csf[i] = cohorts_csf[i].dropna(subset=cohorts_csf[i].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTAU</th>\n",
       "      <th>TAU</th>\n",
       "      <th>ABETA</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>11.81</td>\n",
       "      <td>119.7</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>CU</td>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>33.87</td>\n",
       "      <td>321.3</td>\n",
       "      <td>643.4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>45.14</td>\n",
       "      <td>395.9</td>\n",
       "      <td>651.9</td>\n",
       "      <td>AD</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>58.05</td>\n",
       "      <td>495.7</td>\n",
       "      <td>638.3</td>\n",
       "      <td>MCI</td>\n",
       "      <td>75.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>9.90</td>\n",
       "      <td>114.2</td>\n",
       "      <td>998.6</td>\n",
       "      <td>CU</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>20.75</td>\n",
       "      <td>248.8</td>\n",
       "      <td>666.2</td>\n",
       "      <td>MCI</td>\n",
       "      <td>69.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>15.51</td>\n",
       "      <td>179.7</td>\n",
       "      <td>424.7</td>\n",
       "      <td>AD</td>\n",
       "      <td>81.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>51.87</td>\n",
       "      <td>513.7</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>MCI</td>\n",
       "      <td>74.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>28.64</td>\n",
       "      <td>284.3</td>\n",
       "      <td>783.5</td>\n",
       "      <td>AD</td>\n",
       "      <td>77.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>18.66</td>\n",
       "      <td>218.3</td>\n",
       "      <td>1323.0</td>\n",
       "      <td>CU</td>\n",
       "      <td>74.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PTAU    TAU   ABETA Diagnosis   Age\n",
       "RID                                       \n",
       "5194  11.81  119.7  1386.0        CU  65.2\n",
       "394   33.87  321.3   643.4       MCI  84.0\n",
       "139   45.14  395.9   651.9        AD  65.9\n",
       "855   58.05  495.7   638.3       MCI  75.6\n",
       "4337   9.90  114.2   998.6        CU  72.0\n",
       "...     ...    ...     ...       ...   ...\n",
       "4312  20.75  248.8   666.2       MCI  69.3\n",
       "1062  15.51  179.7   424.7        AD  81.9\n",
       "4712  51.87  513.7  1127.0       MCI  74.3\n",
       "5015  28.64  284.3   783.5        AD  77.8\n",
       "618   18.66  218.3  1323.0        CU  74.8\n",
       "\n",
       "[1017 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohorts_csf['ADNI'].sample(n=len(cohorts_csf['ADNI'].index), replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap, scale features, train model, extract cutoffs and categorize the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_standardization(dfs, cccs, csf_mappings):\n",
    "    \"\"\"\n",
    "    dfs: a dictionary of dataframes where each df containes all CSF measurements, scaled CSF, etc. \n",
    "    cccs: df containing each cluster mean and SD for all CSF biomarkers\n",
    "    csf_mappings: harmonized mapping of CSF biomarkers among the investigated cohorts\n",
    "    \"\"\"\n",
    "    \n",
    "    for study in dfs:\n",
    "        \n",
    "        if '_' not in study:\n",
    "        \n",
    "            for biomarker in csf_mappings['csf'].iloc[:3][study].to_list():\n",
    "                min_, max_ = np.min(dfs[study][biomarker]), np.max(dfs[study][biomarker])\n",
    "                featu = csf_mappings['csf'].loc[csf_mappings['csf'][study]==biomarker, 'Feature'].item()\n",
    "\n",
    "                for prefix in ['mean1_', 'mean2_', 'var1_', 'var2_']:\n",
    "                    cccs.loc[study, prefix + featu + \"_rs\"]= round(cccs.loc[study][prefix + featu] * (max_ - min_) + min_, 2)\n",
    "\n",
    "        else:\n",
    "            study_n = study.split(\"_\")[0]\n",
    "            \n",
    "            for biomarker in csf_mappings['csf'].iloc[:3][study_n].to_list():\n",
    "                min_, max_ = np.min(dfs[study][biomarker]), np.max(dfs[study][biomarker])\n",
    "                featu = csf_mappings['csf'].loc[csf_mappings['csf'][study_n]==biomarker, 'Feature'].item()\n",
    "\n",
    "                for prefix in ['mean1_', 'mean2_', 'var1_', 'var2_']:\n",
    "                    cccs.loc[study, prefix + featu + \"_rs\"]= round(cccs.loc[study][prefix + featu] * (max_ - min_) + min_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(m1, m2, std1, std2):\n",
    "    a = 1/(2*std1**2) - 1/(2*std2**2)\n",
    "    b = m2/(std2**2) - m1/(std1**2)\n",
    "    c = m1**2 /(2*std1**2) - m2**2 / (2*std2**2) - np.log(std2/std1)\n",
    "    return np.roots([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoffs_from_bootstrap_dfs(all_cohorts, iteration_):\n",
    "    \n",
    "    for boot in range(iteration_):\n",
    "        bootstraped_dfs = dict()\n",
    "        my_cols = list()\n",
    "        bio_list = list()\n",
    "        \n",
    "        \n",
    "        # Sample from the data with replacement\n",
    "        for study in all_cohorts:\n",
    "            bootstraped_dfs[study] = all_cohorts[study].sample(n=len(all_cohorts[study].index), replace=True)\n",
    "            bootstraped_dfs[study].to_csv(f\"../results/bootstrap/gmm/datasets/{study}_{boot}.csv\") #save sampled dfs\n",
    "    \n",
    "        # scale each feature and write it as a new column in the respective dataframe\n",
    "        # name the new columns with a suffix \"_scaled\"\n",
    "        for cohort in bootstraped_dfs:\n",
    "\n",
    "            for i in bootstraped_dfs[cohort][bootstraped_dfs[cohort].columns[:3].to_list() + ['Age']]: \n",
    "                bootstraped_dfs[cohort][i + '_scaled'] = MinMaxScaler().fit_transform(X=bootstraped_dfs[cohort][[i]])\n",
    "                \n",
    "                \n",
    "        # add a prefix to each biomarker, for mean and variace obtained from each distribution\n",
    "        for prefix in ['mean1_', 'mean2_', 'var1_', 'var2_']:\n",
    "\n",
    "            for i in mappings['csf']['Feature'][:3].to_list():\n",
    "                my_cols.append(prefix + i)\n",
    "\n",
    "        # make an empty df to save the mean and covariances        \n",
    "        mean_variances_df = pd.DataFrame(index=bootstraped_dfs.keys(), columns=my_cols)\n",
    "        \n",
    "        \n",
    "        # Train GMM with two clusters \n",
    "        for cname in bootstraped_dfs:\n",
    "            \n",
    "            if '_' not in cname:\n",
    "\n",
    "                for col in bootstraped_dfs[cname].columns[:3]:\n",
    "                    variable = mappings['csf'].loc[mappings['csf'][cname]==col, 'Feature'].item()\n",
    "                    # Set the model and its parameters\n",
    "                    model = GaussianMixture(n_components=2, n_init=20)\n",
    "                    # Fit the model \n",
    "                    clm = model.fit(bootstraped_dfs[cname][[col + '_scaled', 'Age_scaled']])\n",
    "                    # extract the mean and std of each Gaussian\n",
    "                    mean_variances_df.loc[cname, 'mean1_' + variable] = clm.means_.tolist()[0][0]\n",
    "                    mean_variances_df.loc[cname, 'mean2_' + variable] = clm.means_.tolist()[1][0]\n",
    "                    # note: we can extract the covariance matrix where the diagonal elements are variance\n",
    "                    # square root of variance will result in std\n",
    "                    mean_variances_df.loc[cname, 'var1_' + variable] = math.sqrt(clm.covariances_.tolist()[0][0][0])\n",
    "                    mean_variances_df.loc[cname, 'var2_' + variable] = math.sqrt(clm.covariances_.tolist()[1][0][0])\n",
    "                    bootstraped_dfs[cname][col + \"_ATN\"] = clm.predict(bootstraped_dfs[cname][[col + '_scaled', 'Age_scaled']])\n",
    "\n",
    "\n",
    "            else:\n",
    "                \n",
    "                cname_new = cname.split(\"_\")[0]\n",
    "\n",
    "                for col in bootstraped_dfs[cname].columns[:3]:\n",
    "                    variable = mappings['csf'].loc[mappings['csf'][cname_new]==col, 'Feature'].item()\n",
    "                    # Set the model and its parameters\n",
    "                    model = GaussianMixture(n_components=2, n_init=20)\n",
    "                    # Fit the model \n",
    "                    clm = model.fit(bootstraped_dfs[cname][[col + '_scaled', 'Age_scaled']])\n",
    "                    # extract the mean and std of each Gaussian\n",
    "                    mean_variances_df.loc[cname, 'mean1_' + variable] = clm.means_.tolist()[0][0]\n",
    "                    mean_variances_df.loc[cname, 'mean2_' + variable] = clm.means_.tolist()[1][0]\n",
    "                    # note: we can extract the covariance matrix where the diagonal elements are variance\n",
    "                    # square root of variance will result in std\n",
    "                    mean_variances_df.loc[cname, 'var1_' + variable] = math.sqrt(clm.covariances_.tolist()[0][0][0])\n",
    "                    mean_variances_df.loc[cname, 'var2_' + variable] = math.sqrt(clm.covariances_.tolist()[1][0][0])\n",
    "                    bootstraped_dfs[cname][col + \"_ATN\"] = clm.predict(bootstraped_dfs[cname][[col + '_scaled', 'Age_scaled']])\n",
    "\n",
    "                \n",
    "        reverse_standardization(bootstraped_dfs, mean_variances_df, mappings)\n",
    "        mean_sd_df = mean_variances_df[[col for col in mean_variances_df.columns if '_rs' in col]]\n",
    "        cutpointssss_GMM = pd.DataFrame(index=bootstraped_dfs.keys(), columns=mappings['csf'].iloc[:3]['Feature'].to_list())\n",
    "        \n",
    "        for ind in bootstraped_dfs:\n",
    "\n",
    "            if \"_\" not in ind:\n",
    "                csf = mappings['csf'].iloc[:3][ind].to_list()\n",
    "                csf_harmon = mappings['csf'].iloc[:3]['Feature'].to_list()\n",
    "\n",
    "                for bioma, bioma_har in zip(csf, csf_harmon):\n",
    "                    m11, std11 = mean_sd_df.loc[ind][['mean1_'+ bioma_har + '_rs', 'var1_'+ bioma_har + '_rs']]\n",
    "                    m22, std22 = mean_sd_df.loc[ind][['mean2_'+ bioma_har + '_rs', 'var2_'+ bioma_har + '_rs']]\n",
    "                    result = solve(m11, m22, std11, std22)\n",
    "                    cutpointssss_GMM.loc[ind, mappings['csf'].loc[mappings['csf'][ind]==bioma, 'Feature']] = round(result[np.argmax(norm.pdf(result, m11, std11))], 2)\n",
    "\n",
    "            else:\n",
    "                csf = mappings['csf'].iloc[:3][ind.split('_')[0]].to_list()\n",
    "                csf_harmon = mappings['csf'].iloc[:3]['Feature'].to_list()\n",
    "\n",
    "                for bioma, bioma_har in zip(csf, csf_harmon):\n",
    "                    m11, std11 = mean_sd_df.loc[ind][['mean1_'+ bioma_har + '_rs', 'var1_'+ bioma_har + '_rs']]\n",
    "                    m22, std22 = mean_sd_df.loc[ind][['mean2_'+ bioma_har + '_rs', 'var2_'+ bioma_har + '_rs']]\n",
    "                    result = solve(m11, m22, std11, std22)\n",
    "                    cutpointssss_GMM.loc[ind, mappings['csf'].loc[mappings['csf'][ind.split('_')[0]]==bioma, 'Feature']] = round(result[np.argmax(norm.pdf(result, m11, std11))], 2)\n",
    "\n",
    "\n",
    "        \n",
    "        # Rearrange the columns to A T N \n",
    "        cutpointssss_GMM = cutpointssss_GMM[['A-beta 1-42 in CSF', 'pTau in CSF', 'tTau in CSF']]\n",
    "        cutpointssss_GMM = cutpointssss_GMM.loc[['ADNI', 'EPAD', 'AIBL', 'ARWIBO', 'EDSD', 'PREVENT-AD', 'PharmaCog', 'NACC_ELISA', 'EMIF_ELISA', 'NACC_XMAP', 'EMIF_XMAP', 'DOD-ADNI', 'JADNI']]\n",
    "        cutpointssss_GMM.to_csv(f\"../results/bootstrap/gmm/cutoffs/gmm_cutoffs_{boot}.csv\") # save cutoffs\n",
    "        \n",
    "        \n",
    "        for a in ['A', 'T', 'N']: \n",
    "            for b in ['+', '-']:\n",
    "                bio_list.append(a+b)\n",
    "        \n",
    "        profiles_df = pd.DataFrame(index=cutpointssss_GMM.index, columns=bio_list)\n",
    "        classes = {i: pd.DataFrame(index=all_cohorts[i].index, columns=['A', 'T', 'N']) for i in cutpointssss_GMM.index}\n",
    "        for i in classes: classes[i].replace({np.nan: 0}, inplace=True)\n",
    "            \n",
    "            \n",
    "        for ind in cutpointssss_GMM.index:\n",
    "    \n",
    "            if \"_\" not in ind:\n",
    "\n",
    "                for col, letter in zip(cutpointssss_GMM.columns, ['A', 'T', 'N']):\n",
    "                    threshold = cutpointssss_GMM.loc[ind][col]\n",
    "                    bio = mappings['csf'].loc[mappings['csf']['Feature']==col, ind].item()\n",
    "\n",
    "                    if letter == 'T':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"T\"] = 1\n",
    "                    \n",
    "                    elif letter == 'N':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"N\"] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]<threshold].index, \"A\"] = 1\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                 for col, letter in zip(cutpointssss_GMM.columns, ['A', 'T', 'N']):\n",
    "                    threshold = cutpointssss_GMM.loc[ind][col]\n",
    "                    bio = mappings['csf'].loc[mappings['csf']['Feature']==col, ind.split(\"_\")[0]].item()\n",
    "\n",
    "                    if letter == 'T': \n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"T\"] = 1\n",
    "                    \n",
    "                    elif letter == 'N':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"N\"] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]<threshold].index, \"A\"] = 1\n",
    "                        \n",
    "        for i in classes:\n",
    "            classes[i]['ATN'] = classes[i]['A'].astype(str) + classes[i]['T'].astype(str) + classes[i]['N'].astype(str)\n",
    "\n",
    "        final_profiles = pd.DataFrame(index=classes, columns=list(Counter(classes['ADNI']['ATN']).keys()))\n",
    "        final_profiles.replace({np.nan: 0}, inplace=True)\n",
    "\n",
    "        for i in classes:\n",
    "            profs = dict(Counter(classes[i]['ATN']))\n",
    "\n",
    "            for pro in profs:\n",
    "                final_profiles.loc[i, pro] = profs[pro]\n",
    "        \n",
    "        final_profiles.rename(columns={'000': \"A-T-N-\", '100': 'A+T-N-', '111': 'A+T+N+', '110': 'A+T+N-', \n",
    "                               '011': \"A-T+N+\", '101': \"A+T-N+\", '001': 'A-T-N+', '010': 'A-T+N-'}, inplace=True)\n",
    "        final_profiles = final_profiles[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A-T+N-', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]\n",
    "        final_profiles.replace({np.nan: 0}, inplace=True)\n",
    "        final_profiles.loc['NACC'] = final_profiles.loc['NACC_ELISA'] + final_profiles.loc['NACC_XMAP']\n",
    "        final_profiles.loc['EMIF'] = final_profiles.loc['EMIF_ELISA'] + final_profiles.loc['EMIF_XMAP'] \n",
    "        final_profiles = final_profiles.loc[['ADNI', 'EPAD', 'AIBL', 'ARWIBO', 'EDSD', 'PREVENT-AD', 'PharmaCog', 'NACC', 'EMIF', 'DOD-ADNI', 'JADNI']]\n",
    "        final_profiles.to_csv(f\"../results/bootstrap/gmm/profiles/gmm_profiles_{boot}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs_from_bootstrap_dfs(cohorts_csf, iteration_=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
