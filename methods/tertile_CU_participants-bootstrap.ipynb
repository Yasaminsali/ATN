{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read every cohort study file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../cohort_studies_full_data/' + \"/*.\"+'csv'))]\n",
    "cohorts = [file.split(\".\")[0] for file in sorted(os.listdir('../cohort_studies_full_data/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies = dict()\n",
    "# dfsss = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts, datasets):\n",
    "    cohort_n = cohort.split(\"_MERGE\")[0]\n",
    "    cohort_studies[cohort_n] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit\n",
    "#     dfsss[cohort_n] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_sub = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../preprocessed_datasets/' + \"/*.\"+'csv'))]\n",
    "cohorts_sub = [file.split(\".\")[0] for file in sorted(os.listdir('../preprocessed_datasets/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies_sub = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts_sub, datasets_sub):\n",
    "    cohort_studies_sub[cohort] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cohort_studies:\n",
    "    cohort_studies[i]['Age']=cohort_studies_sub[i]['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read harmonized mapping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = [pd.read_csv(file, sep=',') for file in sorted(glob.glob('../feature_tables' + \"/*.\"+'csv'))]\n",
    "name = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_tables'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "mappings = dict()\n",
    "\n",
    "for moda, na in zip(modality, name):\n",
    "    mappings[na.split(' - ')[1]] = moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_features = pd.concat(mappings, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude categorical and taboo features\n",
    "harmonized_features = harmonized_features.loc[(harmonized_features['Rank']!=1) & (harmonized_features['Rank']!=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the feature availability files for all cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_mapp = [pd.read_csv(file, sep='\\t') for file in sorted(glob.glob('../feature_availability_in_cohorts' + \"/*.\"+'tsv'))]\n",
    "tablesss = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_availability_in_cohorts'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "available_features = dict()\n",
    "\n",
    "for modal, df in zip(tablesss, ava_mapp):\n",
    "    available_features[modal] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features = pd.concat(available_features, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features.replace({0: np.nan}, inplace=True) # 0 indicates that the feature was not measured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecetion of cohort studies for A/T/N assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the patient that have CSF biomarker, disregard the diagnostic status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "atn = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=mappings['csf'].Feature.loc[0:2].to_list()+([\"Total\"]))\n",
    "# atn = pd.DataFrame(index=cohort_studies, columns=['A', 'T', 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    for feat in mappings['csf'][cohort].loc[0:2].dropna().to_list():\n",
    "        if feat in cohort_studies[cohort].columns:\n",
    "            atn.loc[cohort, mappings['csf'].loc[mappings['csf'][cohort]==feat, 'Feature']] = len(cohort_studies[cohort][feat].dropna())\n",
    "            atn.loc[cohort, 'Total'] = len(cohort_studies[cohort][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=cohort_studies['ADNI']['Diagnosis'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in diag.index:\n",
    "    for dia in diag.columns:\n",
    "        diag.loc[cohort, dia] = len(cohort_studies[cohort].loc[cohort_studies[cohort]['Diagnosis']==dia][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the empty columns from all cohorts that we are intrested in\n",
    "### Remove the participant without all 3 CSF biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cohorts = dict()\n",
    "\n",
    "for coh in diag.index:\n",
    "    selected_cohorts[coh] = cohort_studies[coh].dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feats = dict()\n",
    "\n",
    "# existing_features.set_index('Feature', inplace=True)\n",
    "\n",
    "for feat in existing_features.Feature:\n",
    "    total_feats[feat] = existing_features.loc[existing_features.Feature==feat][selected_cohorts].dropna(axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    feat = mappings['csf'][cohort].loc[0:2].dropna().to_list()\n",
    "    cohort_studies[cohort] = cohort_studies[cohort].dropna(subset=feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Some features have suffix due to merging tables for certain cohorts, first investigate if all the harmonized features are in cohorts. Rename the ones that have suffix so it can be compatible to work with our harmonized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_studies['ADNI'].rename(columns={'PTEDUCAT_x': 'PTEDUCAT', 'TRABSCOR_bl': 'TRABSCOR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the CSF biomarkers in different cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(11, 3, sharex=False, figsize=(20, 35))\n",
    "# fig.subplots_adjust(hspace=0.7)\n",
    "# i=0\n",
    "\n",
    "# for ind in atn.index:\n",
    "#     csf = mappings['csf'].iloc[:3][ind].to_list()\n",
    "#     colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "#     for bioma, colo in zip(csf, colors):\n",
    "# #         print(bioma, csf.index(bioma))\n",
    "#         sns.histplot(cohort_studies[ind][bioma].dropna(), ax=axes[i, csf.index(bioma)], color=colo)\n",
    "# #         axes[i, csf.index(bioma)].set_title(bioma.upper())\n",
    "#         axes[i, 1].set_title(ind, fontsize=16, pad=10)\n",
    "    \n",
    "#     i+=1\n",
    "    \n",
    "# # plt.savefig(\"csf_biomarkers.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(11, 3, sharex=False, figsize=(20, 35))\n",
    "# fig.subplots_adjust(hspace=0.7)\n",
    "# i=0\n",
    "\n",
    "# for ind in atn.index:\n",
    "#     csf = mappings['csf'].iloc[:3][ind].to_list()\n",
    "#     colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "#     for bioma, colo in zip(csf, colors):\n",
    "# #         print(bioma, csf.index(bioma))\n",
    "#         sns.scatterplot(x=cohort_studies[ind]['Age'], y=cohort_studies[ind][bioma].dropna(), ax=axes[i, csf.index(bioma)], color=colo)\n",
    "# #         axes[i, csf.index(bioma)].set_title(bioma.upper())\n",
    "#         axes[i, 1].set_title(ind, fontsize=16, pad=10)\n",
    "    \n",
    "#     i+=1\n",
    "    \n",
    "# # plt.savefig(\"csf_biomarkers.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSF biomarkers, two classes, normal vs abnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset each cohort dataset based on the columns of interest for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts_csf = dict()\n",
    "\n",
    "for i in atn.index:\n",
    "    \n",
    "    if len(cohort_studies[i].loc[cohort_studies[i]['Diagnosis']=='CU'].index)>2:\n",
    "        csf = mappings['csf'].iloc[:3][i].to_list()\n",
    "    \n",
    "        if i == 'NACC':\n",
    "            cohorts_csf['NACC_ELISA'] = cohort_studies[i].loc[cohort_studies[i]['CSFTTMD']==1][csf + [\"Diagnosis\", \"Age\"]] # ELISA\n",
    "            cohorts_csf['NACC_XMAP'] = cohort_studies[i].loc[cohort_studies[i]['CSFTTMD']==2][csf + [\"Diagnosis\", \"Age\"]] # xmap\n",
    "            cohorts_csf['NACC_ELISA'] = cohorts_csf['NACC_ELISA'].dropna(subset=cohorts_csf['NACC_ELISA'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "            cohorts_csf['NACC_XMAP'] = cohorts_csf['NACC_XMAP'].dropna(subset=cohorts_csf['NACC_XMAP'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "\n",
    "\n",
    "        elif i == 'EMIF':\n",
    "            cohorts_csf['EMIF_ELISA'] = cohort_studies[i].loc[~(cohort_studies[i]['Studyname'].isin(['EDAR', 'Leuven', ]))][csf + [\"Diagnosis\", \"Age\"]] # INNOTEST ELISA\n",
    "            cohorts_csf['EMIF_XMAP'] = cohort_studies[i].loc[(cohort_studies[i]['Studyname'].isin(['EDAR', 'Leuven', ]))][csf + [\"Diagnosis\", \"Age\"]] # xmap and not collected\n",
    "            cohorts_csf['EMIF_ELISA'] = cohorts_csf['EMIF_ELISA'].dropna(subset=cohorts_csf['EMIF_ELISA'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "            cohorts_csf['EMIF_XMAP'] = cohorts_csf['EMIF_XMAP'].dropna(subset=cohorts_csf['EMIF_XMAP'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "\n",
    "        else: \n",
    "            cohorts_csf[i] = cohort_studies[i][csf + [\"Diagnosis\", \"Age\"]]\n",
    "            cohorts_csf[i] = cohorts_csf[i].dropna(subset=cohorts_csf[i].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts_cu = dict()\n",
    "\n",
    "for i in cohorts_csf:\n",
    "    cohorts_cu[i] = cohorts_csf[i].loc[cohorts_csf[i]['Diagnosis']=='CU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ADNI', 'DOD-ADNI', 'EMIF_ELISA', 'EMIF_XMAP', 'EPAD', 'JADNI', 'NACC_ELISA', 'NACC_XMAP', 'PREVENT-AD'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohorts_cu.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap, scale features, train model, extract cutoffs and categorize the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tertile_cutoffs(dfs, csfs, result):\n",
    "    \"\"\"dfs: a dictionary containing dataframes of all cohort studies.\n",
    "       csfs: a dictionary of mapping with \"csf\" as the key for the dataframe of mapped csf biomarkers\n",
    "       result: dataframe where cohort names are indices and csf biomarkers are columns\n",
    "       output: calculate the cutoff values based on tertile where, the cutoff for abeta is first interval and \n",
    "       for ttau and ptau the cutoff is the second interval value.\n",
    "    \"\"\"\n",
    "    csf_bioma = csfs['csf'].iloc[:3]['Feature'].to_list()\n",
    "    \n",
    "    for coh_name in dfs:\n",
    "        \n",
    "        if '_' not in coh_name:\n",
    "\n",
    "            for biomark in csf_bioma:\n",
    "\n",
    "                if biomark =='A-beta 1-42 in CSF':\n",
    "                    columnn = csfs['csf'].loc[csfs['csf']['Feature']==biomark][coh_name].item()\n",
    "                    result.loc[coh_name, biomark] = [round(x, 2) for x in list(pd.qcut(dfs[coh_name][columnn], 3, retbins=True)[1][1:3])][0]\n",
    "\n",
    "                else:\n",
    "                    columnn = csfs['csf'].loc[csfs['csf']['Feature']==biomark][coh_name].item()\n",
    "                    result.loc[coh_name, biomark] = [round(x, 2) for x in list(pd.qcut(dfs[coh_name][columnn], 3, retbins=True)[1][1:3])][1]       \n",
    "                    \n",
    "        else:\n",
    "\n",
    "            for biomark in csf_bioma:\n",
    "\n",
    "                if biomark =='A-beta 1-42 in CSF':\n",
    "                    columnn = csfs['csf'].loc[csfs['csf']['Feature']==biomark][coh_name.split('_')[0]].item()\n",
    "                    result.loc[coh_name, biomark] = [round(x, 2) for x in list(pd.qcut(dfs[coh_name][columnn], 3, retbins=True)[1][1:3])][0]\n",
    "\n",
    "                else:\n",
    "                    columnn = csfs['csf'].loc[csfs['csf']['Feature']==biomark][coh_name.split('_')[0]].item()\n",
    "                    result.loc[coh_name, biomark] = [round(x, 2) for x in list(pd.qcut(dfs[coh_name][columnn], 3, retbins=True)[1][1:3])][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoffs_from_bootstrap_dfs(all_cohorts_cu, all_cohorts, iteration_):\n",
    "    \n",
    "    for boot in range(iteration_):\n",
    "        bootstraped_dfs = dict()\n",
    "        bio_list = list()\n",
    "        cutpointssss = pd.DataFrame(index=all_cohorts_cu.keys(), columns=mappings['csf'].iloc[:3]['Feature'].to_list())\n",
    "        \n",
    "        # Sample from the data with replacement\n",
    "        for study in all_cohorts:\n",
    "            bootstraped_dfs[study] = all_cohorts_cu[study].sample(n=len(all_cohorts_cu[study].index), replace=True)\n",
    "            bootstraped_dfs[study].to_csv(f\"../results/bootstrap/tertile/datasets/{study}_{boot}.csv\") #save sampled\n",
    "            \n",
    "        tertile_cutoffs(bootstraped_dfs, mappings, cutpointssss)\n",
    "        \n",
    "        # Rearrange the columns to A T N \n",
    "        cutpointssss = cutpointssss[['A-beta 1-42 in CSF', 'pTau in CSF', 'tTau in CSF']]\n",
    "        cutpointssss = cutpointssss.loc[['ADNI', 'EPAD', 'PREVENT-AD', 'NACC_ELISA', 'EMIF_ELISA', 'NACC_XMAP', 'EMIF_XMAP', 'DOD-ADNI', 'JADNI']]\n",
    "        cutpointssss.to_csv(f\"../results/bootstrap/tertile/cutoffs/tertile_cutoffs_{boot}.csv\")\n",
    "        \n",
    "        for a in ['A', 'T', 'N']: \n",
    "            for b in ['+', '-']:\n",
    "                bio_list.append(a+b)\n",
    "\n",
    "        profiles_df = pd.DataFrame(index=cutpointssss.index, columns=bio_list)\n",
    "\n",
    "        classes = {i: pd.DataFrame(index=all_cohorts[i].index, columns=['A', 'T', 'N']) for i in cutpointssss.index}\n",
    "        for i in classes: classes[i].replace({np.nan: 0}, inplace=True)\n",
    "            \n",
    "        for ind in cutpointssss.index:\n",
    "\n",
    "            if \"_\" not in ind:\n",
    "\n",
    "                for col, letter in zip(cutpointssss.columns, ['A', 'T', 'N']):\n",
    "                    threshold = cutpointssss.loc[ind][col]\n",
    "                    bio = mappings['csf'].loc[mappings['csf']['Feature']==col, ind].item()\n",
    "\n",
    "                    if letter == 'T':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"T\"] = 1\n",
    "                    \n",
    "                    elif letter == 'N':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"N\"] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]<threshold].index, \"A\"] = 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                for col, letter in zip(cutpointssss.columns, ['A', 'T', 'N']):\n",
    "                    threshold = cutpointssss.loc[ind][col]\n",
    "                    bio = mappings['csf'].loc[mappings['csf']['Feature']==col, ind.split(\"_\")[0]].item()\n",
    "\n",
    "                    if letter == 'T':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"T\"] = 1\n",
    "                    \n",
    "                    elif letter == 'N':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"N\"] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]<threshold].index, \"A\"] = 1\n",
    "\n",
    "\n",
    "        for i in classes:\n",
    "            classes[i]['ATN'] = classes[i]['A'].astype(str) + classes[i]['T'].astype(str) + classes[i]['N'].astype(str)\n",
    "\n",
    "        final_profiles = pd.DataFrame(index=classes, columns=list(Counter(classes['ADNI']['ATN']).keys()))\n",
    "        final_profiles.replace({np.nan: 0}, inplace=True)\n",
    "        for i in classes:\n",
    "            profs = dict(Counter(classes[i]['ATN']))\n",
    "\n",
    "            for pro in profs:\n",
    "                final_profiles.loc[i, pro] = profs[pro]\n",
    "\n",
    "        final_profiles.rename(columns={'000': \"A-T-N-\", '100': 'A+T-N-', '111': 'A+T+N+', '110': 'A+T+N-', \n",
    "                                       '011': \"A-T+N+\", '101': \"A+T-N+\", '001': 'A-T-N+', '010': 'A-T+N-'}, inplace=True)\n",
    "        final_profiles = final_profiles[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A-T+N-', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]\n",
    "        final_profiles.replace({np.nan: 0}, inplace=True)\n",
    "        final_profiles.loc['NACC'] = final_profiles.loc['NACC_ELISA'] + final_profiles.loc['NACC_XMAP']\n",
    "        final_profiles.loc['EMIF'] = final_profiles.loc['EMIF_ELISA'] + final_profiles.loc['EMIF_XMAP'] \n",
    "        final_profiles = final_profiles.loc[['ADNI', 'EPAD', 'PREVENT-AD', 'NACC', 'EMIF', 'DOD-ADNI', 'JADNI']]\n",
    "        final_profiles.to_csv(f\"../results/bootstrap/tertile/profiles/final_profiles_tertile_{boot}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs_from_bootstrap_dfs(cohorts_cu, cohorts_csf, iteration_=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
