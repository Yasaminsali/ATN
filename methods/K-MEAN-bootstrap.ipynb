{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read every cohort study file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../cohort_studies_full_data/' + \"/*.\"+'csv'))]\n",
    "cohorts = [file.split(\".\")[0] for file in sorted(os.listdir('../cohort_studies_full_data/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies = dict()\n",
    "# dfsss = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts, datasets):\n",
    "    cohort_n = cohort.split(\"_MERGE\")[0]\n",
    "    cohort_studies[cohort_n] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit\n",
    "#     dfsss[cohort_n] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_sub = [pd.read_csv(file, index_col=0, low_memory=False) for file in sorted(glob.glob('../preprocessed_datasets/' + \"/*.\"+'csv'))]\n",
    "cohorts_sub = [file.split(\".\")[0] for file in sorted(os.listdir('../preprocessed_datasets/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all cohorts as a dataframe\n",
    "cohort_studies_sub = dict()\n",
    "\n",
    "for cohort, dataset in zip(cohorts_sub, datasets_sub):\n",
    "    cohort_studies_sub[cohort] = dataset.loc[dataset['Months']==0].copy() # reduce to BL visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cohort_studies:\n",
    "    cohort_studies[i]['Age']=cohort_studies_sub[i]['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read harmonized mapping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = [pd.read_csv(file, sep=',') for file in sorted(glob.glob('../feature_tables' + \"/*.\"+'csv'))]\n",
    "name = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_tables'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "mappings = dict()\n",
    "\n",
    "for moda, na in zip(modality, name):\n",
    "    mappings[na.split(' - ')[1]] = moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_features = pd.concat(mappings, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude categorical and taboo features\n",
    "harmonized_features = harmonized_features.loc[(harmonized_features['Rank']!=1) & (harmonized_features['Rank']!=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the feature availability files for all cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_mapp = [pd.read_csv(file, sep='\\t') for file in sorted(glob.glob('../feature_availability_in_cohorts' + \"/*.\"+'tsv'))]\n",
    "tablesss = [file.split(\".\")[0] for file in sorted(os.listdir('../feature_availability_in_cohorts'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that contains all modalities as a dataframe\n",
    "available_features = dict()\n",
    "\n",
    "for modal, df in zip(tablesss, ava_mapp):\n",
    "    available_features[modal] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features = pd.concat(available_features, ignore_index=True) # combine all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features.replace({0: np.nan}, inplace=True) # 0 indicates that the feature was not measured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecetion of cohort studies for A/T/N assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the patient that have CSF biomarker, disregard the diagnostic status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "atn = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=mappings['csf'].Feature.loc[0:2].to_list()+([\"Total\"]))\n",
    "# atn = pd.DataFrame(index=cohort_studies, columns=['A', 'T', 'N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    for feat in mappings['csf'][cohort].loc[0:2].dropna().to_list():\n",
    "        if feat in cohort_studies[cohort].columns:\n",
    "            atn.loc[cohort, mappings['csf'].loc[mappings['csf'][cohort]==feat, 'Feature']] = len(cohort_studies[cohort][feat].dropna())\n",
    "            atn.loc[cohort, 'Total'] = len(cohort_studies[cohort][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.DataFrame(index=available_features['csf'].iloc[:3].replace({0: np.nan}).dropna(axis=1).columns[1:].to_list(), columns=cohort_studies['ADNI']['Diagnosis'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in diag.index:\n",
    "    for dia in diag.columns:\n",
    "        diag.loc[cohort, dia] = len(cohort_studies[cohort].loc[cohort_studies[cohort]['Diagnosis']==dia][mappings['csf'][cohort].loc[0:2].dropna().to_list()].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the empty columns from all cohorts that we are intrested in\n",
    "### Remove the participant without all 3 CSF biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cohorts = dict()\n",
    "\n",
    "for coh in diag.index:\n",
    "    selected_cohorts[coh] = cohort_studies[coh].dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feats = dict()\n",
    "\n",
    "# existing_features.set_index('Feature', inplace=True)\n",
    "\n",
    "for feat in existing_features.Feature:\n",
    "    total_feats[feat] = existing_features.loc[existing_features.Feature==feat][selected_cohorts].dropna(axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in atn.index:\n",
    "    feat = mappings['csf'][cohort].loc[0:2].dropna().to_list()\n",
    "    cohort_studies[cohort] = cohort_studies[cohort].dropna(subset=feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Some features have suffix due to merging tables for certain cohorts, first investigate if all the harmonized features are in cohorts. Rename the ones that have suffix so it can be compatible to work with our harmonized names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_studies['ADNI'].rename(columns={'PTEDUCAT_x': 'PTEDUCAT', 'TRABSCOR_bl': 'TRABSCOR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering CSF biomarkers, two classes, normal vs abnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset each cohort dataset based on the columns of interest for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts_csf = dict()\n",
    "\n",
    "for i in atn.index:\n",
    "    csf = mappings['csf'].iloc[:3][i].to_list()\n",
    "    \n",
    "    if i == 'NACC':\n",
    "        cohorts_csf['NACC_ELISA'] = cohort_studies[i].loc[cohort_studies[i]['CSFTTMD']==1][csf + [\"Diagnosis\", \"Age\"]] # ELISA\n",
    "        cohorts_csf['NACC_XMAP'] = cohort_studies[i].loc[cohort_studies[i]['CSFTTMD']==2][csf + [\"Diagnosis\", \"Age\"]] # xmap\n",
    "        cohorts_csf['NACC_ELISA'] = cohorts_csf['NACC_ELISA'].dropna(subset=cohorts_csf['NACC_ELISA'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "        cohorts_csf['NACC_XMAP'] = cohorts_csf['NACC_XMAP'].dropna(subset=cohorts_csf['NACC_XMAP'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "\n",
    "    \n",
    "    elif i == 'EMIF':\n",
    "        cohorts_csf['EMIF_ELISA'] = cohort_studies[i].loc[~(cohort_studies[i]['Studyname'].isin(['EDAR', 'Leuven', ]))][csf + [\"Diagnosis\", \"Age\"]] # INNOTEST ELISA\n",
    "        cohorts_csf['EMIF_XMAP'] = cohort_studies[i].loc[(cohort_studies[i]['Studyname'].isin(['EDAR', 'Leuven', ]))][csf + [\"Diagnosis\", \"Age\"]] # xmap and not collected\n",
    "        cohorts_csf['EMIF_ELISA'] = cohorts_csf['EMIF_ELISA'].dropna(subset=cohorts_csf['EMIF_ELISA'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "        cohorts_csf['EMIF_XMAP'] = cohorts_csf['EMIF_XMAP'].dropna(subset=cohorts_csf['EMIF_XMAP'].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)\n",
    "\n",
    "    else: \n",
    "        cohorts_csf[i] = cohort_studies[i][csf + [\"Diagnosis\", \"Age\"]]\n",
    "        cohorts_csf[i] = cohorts_csf[i].dropna(subset=cohorts_csf[i].columns[:3].to_list() + ['Age']) # drop empty rows (CSF biomarkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap, scale features, train model, extract cutoffs and categorize the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_standardization(dfs, cccs, csf_mappings):\n",
    "    \"\"\"\n",
    "    dfs: a dictionary of dataframes where each df containes all CSF measurements, scaled CSF, etc. \n",
    "    cccs: df containing all cluster centers for all CSF biomarkers\n",
    "    csf_mappings: harmonized mapping of CSF biomarkers among the investigated cohorts\n",
    "    \"\"\"\n",
    "    \n",
    "    for study in dfs:\n",
    "        \n",
    "        if '_' not in study:\n",
    "\n",
    "            for biomarker in csf_mappings['csf'].iloc[:3][study].to_list():\n",
    "\n",
    "                min_, max_ = np.min(dfs[study][biomarker]), np.max(dfs[study][biomarker])\n",
    "                featu = csf_mappings['csf'].loc[csf_mappings['csf'][study]==biomarker, 'Feature']\n",
    "                cccs.loc[study, featu + \"_threshold\"]= round(cccs.loc[study][featu].item() * (max_ - min_) + min_, 2)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            for biomarker in csf_mappings['csf'].iloc[:3][study.split(\"_\")[0]].to_list():\n",
    "\n",
    "                min_, max_ = np.min(dfs[study][biomarker]), np.max(dfs[study][biomarker])\n",
    "                featu = csf_mappings['csf'].loc[csf_mappings['csf'][study.split(\"_\")[0]]==biomarker, 'Feature']\n",
    "                cccs.loc[study, featu + \"_threshold\"]= round(cccs.loc[study][featu].item() * (max_ - min_) + min_, 2)  \n",
    "            \n",
    "    return cccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoffs_from_bootstrap_dfs(all_cohorts, iteration_):\n",
    "    \n",
    "    for boot in range(iteration_):\n",
    "        bootstraped_dfs = dict()\n",
    "        bio_list = list()\n",
    "        \n",
    "        centroids = pd.DataFrame(index=all_cohorts, columns= mappings['csf'].loc[:2]['Feature'].to_list())\n",
    "        \n",
    "        # Sample from the data with replacement\n",
    "        for study in all_cohorts:\n",
    "            bootstraped_dfs[study] = all_cohorts[study].sample(n=len(all_cohorts[study].index), replace=True)\n",
    "            bootstraped_dfs[study].to_csv(f\"../results/bootstrap/kmeans/datasets/{study}_{boot}.csv\") #save sampled dfs\n",
    "            \n",
    "        # scale each feature and write it as a new column in the respective dataframe\n",
    "        # name the new columns with a suffix \"_scaled\"\n",
    "        for cohort in bootstraped_dfs:\n",
    "    \n",
    "            for i in bootstraped_dfs[cohort][bootstraped_dfs[cohort].columns[:3].to_list() + ['Age']]: \n",
    "                bootstraped_dfs[cohort][i + '_scaled'] = MinMaxScaler().fit_transform(X=bootstraped_dfs[cohort][[i]])\n",
    "\n",
    "        # Train k-means model with two clusters\n",
    "        for coh in bootstraped_dfs:\n",
    "    \n",
    "            if \"_\" not in coh:\n",
    "\n",
    "                for biom in [x + \"_scaled\" for x in mappings['csf'].iloc[:3][coh].to_list()]:\n",
    "\n",
    "                    # Set the model and its parameters\n",
    "                    model = KMeans(n_clusters=2, n_init=20)\n",
    "                    # Fit the model \n",
    "                    clm = model.fit(bootstraped_dfs[coh][[biom, 'Age_scaled']])\n",
    "                    centroids.loc[coh, mappings['csf'].loc[mappings['csf'][coh]==biom.split('_scaled')[0], 'Feature']] = np.average((clm.cluster_centers_[0].tolist(), clm.cluster_centers_[1].tolist()), axis=0).tolist()[0]\n",
    "                    bootstraped_dfs[coh][biom.split('_scaled')[0] + \"_ATN\"] = clm.predict(bootstraped_dfs[coh][[biom, 'Age_scaled']])\n",
    "\n",
    "            else:\n",
    "\n",
    "                for biom in [x + \"_scaled\" for x in mappings['csf'].iloc[:3][coh.split(\"_\")[0]].to_list()]:\n",
    "\n",
    "                    # Set the model and its parameters\n",
    "                    model = KMeans(n_clusters=2, n_init=20)\n",
    "                    # Fit the model \n",
    "                    clm = model.fit(bootstraped_dfs[coh][[biom, 'Age_scaled']])\n",
    "                    centroids.loc[coh, mappings['csf'].loc[mappings['csf'][coh.split(\"_\")[0]]==biom.split('_scaled')[0], 'Feature']] = np.average((clm.cluster_centers_[0].tolist(), clm.cluster_centers_[1].tolist()), axis=0).tolist()[0]\n",
    "                    bootstraped_dfs[coh][biom.split('_scaled')[0] + \"_ATN\"] = clm.predict(bootstraped_dfs[coh][[biom, 'Age_scaled']])\n",
    "        \n",
    "        # reverse the standardize values in order to know the true cutoff\n",
    "        centroids = reverse_standardization(bootstraped_dfs, centroids, mappings)\n",
    "        centroids = centroids.loc[['ADNI', 'EPAD', 'AIBL', 'ARWIBO', 'EDSD', 'PREVENT-AD', 'PharmaCog', 'NACC_ELISA', 'EMIF_ELISA', 'NACC_XMAP', 'EMIF_XMAP', 'DOD-ADNI', 'JADNI']]\n",
    "        centroids = centroids[['A-beta 1-42 in CSF_threshold', 'pTau in CSF_threshold', 'tTau in CSF_threshold']]\n",
    "        centroids.to_csv(f\"../results/bootstrap/kmeans/cutoffs/km_cutoffs_{boot}.csv\")\n",
    "        \n",
    "        for a in ['A', 'T', 'N']: \n",
    "            for b in ['+', '-']:\n",
    "                bio_list.append(a+b)\n",
    "\n",
    "        profiles_df = pd.DataFrame(index=centroids.index, columns=bio_list)\n",
    "        classes = {i: pd.DataFrame(index=all_cohorts[i].index, columns=['A', 'T', 'N']) for i in centroids.index}\n",
    "        for i in classes: classes[i].replace({np.nan: 0}, inplace=True)\n",
    "  \n",
    "        for ind in centroids.index:\n",
    "    \n",
    "            if \"_\" not in ind:\n",
    "\n",
    "                for col, letter in zip(centroids.columns[:3], ['A', 'T', 'N']):\n",
    "                    threshold = centroids.loc[ind][col]\n",
    "                    bio = mappings['csf'].loc[mappings['csf']['Feature']==col.split(\"_threshold\")[0], ind].item()\n",
    "\n",
    "                    if letter == 'T':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"T\"] = 1\n",
    "                    \n",
    "                    elif letter == 'N':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"N\"] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]<threshold].index, \"A\"] = 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                 for col, letter in zip(centroids.columns[:3], ['A', 'T', 'N']):\n",
    "                    threshold = centroids.loc[ind][col]\n",
    "                    bio = mappings['csf'].loc[mappings['csf']['Feature']==col.split(\"_threshold\")[0], ind.split(\"_\")[0]].item()\n",
    "\n",
    "                    if letter == 'T':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"T\"] = 1\n",
    "                    \n",
    "                    elif letter == 'N':\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]>threshold].index, \"N\"] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        classes[ind].loc[all_cohorts[ind].loc[all_cohorts[ind][bio]<threshold].index, \"A\"] = 1\n",
    "                        \n",
    "\n",
    "        for i in classes:\n",
    "            classes[i]['ATN'] = classes[i]['A'].astype(str) + classes[i]['T'].astype(str) + classes[i]['N'].astype(str)\n",
    "\n",
    "        final_profiles = pd.DataFrame(index=classes, columns=list(Counter(classes['ADNI']['ATN']).keys()))\n",
    "\n",
    "        for i in classes:\n",
    "            profs = dict(Counter(classes[i]['ATN']))\n",
    "\n",
    "            for pro in profs:\n",
    "                final_profiles.loc[i, pro] = profs[pro]\n",
    "\n",
    "        final_profiles.rename(columns={'000': \"A-T-N-\", '100': 'A+T-N-', '111': 'A+T+N+', '110': 'A+T+N-', \n",
    "                                       '011': \"A-T+N+\", '101': \"A+T-N+\", '001': 'A-T-N+', '010': 'A-T+N-'}, inplace=True)\n",
    "        final_profiles.replace({np.nan: 0}, inplace=True)\n",
    "        final_profiles = final_profiles[['A-T-N-', 'A-T+N+', 'A-T-N+', 'A-T+N-', 'A+T+N-', 'A+T-N-', 'A+T-N+', 'A+T+N+']]\n",
    "        final_profiles.replace({np.nan: 0}, inplace=True)\n",
    "        final_profiles.loc['NACC'] = final_profiles.loc['NACC_ELISA'] + final_profiles.loc['NACC_XMAP']\n",
    "        final_profiles.loc['EMIF'] = final_profiles.loc['EMIF_ELISA'] + final_profiles.loc['EMIF_XMAP']\n",
    "        final_profiles = final_profiles.loc[['ADNI', 'EPAD', 'AIBL', 'ARWIBO', 'EDSD', 'PREVENT-AD', 'PharmaCog', 'NACC', 'EMIF', 'DOD-ADNI', 'JADNI']]\n",
    "        final_profiles.astype(int).to_csv(f\"../results/bootstrap/kmeans/profiles/final_profiles_kmeans_{boot}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs_from_bootstrap_dfs(cohorts_csf, iteration_=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
